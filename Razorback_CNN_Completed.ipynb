{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41d3072",
   "metadata": {},
   "source": [
    "# DASC 41103 – Project 3: CNN Razorback Logo Classifier (Colab Version)\n",
    "\n",
    "This notebook is modeled after the class CNN examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 1: Mount Google Drive (Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Drive mounted successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab - skipping drive.mount().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 2: Imports and basic configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "SEED=42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_ROOT = Path(\"/content/drive/MyDrive/DEIN_ORDNER/dataset\")\n",
    "TRAIN_RATIO=0.8\n",
    "BATCH_SIZE=8\n",
    "NUM_EPOCHS=15\n",
    "LEARNING_RATE=1e-3\n",
    "GROUP_NUMBER=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 3: Check folders\n",
    "hogs_dir=DATASET_ROOT/'hogs'\n",
    "no_hogs_dir=DATASET_ROOT/'no_hogs'\n",
    "print(hogs_dir, no_hogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 4: Transforms and dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((500,500)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "base_dataset = datasets.ImageFolder(root=DATASET_ROOT, transform=transform)\n",
    "print(base_dataset.classes, base_dataset.class_to_idx, len(base_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 5: Wrapper dataset\n",
    "class HogLogoDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds=ds\n",
    "        m=ds.class_to_idx\n",
    "        self.map={m['hogs']:1, m['no_hogs']:0}\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        x,y=self.ds[i]\n",
    "        return x, self.map[y]\n",
    "\n",
    "full_dataset=HogLogoDataset(base_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 6: Split\n",
    "total=len(full_dataset)\n",
    "train_size=int(TRAIN_RATIO*total)\n",
    "valid_size=total-train_size\n",
    "train_dataset, valid_dataset = random_split(full_dataset,[train_size,valid_size])\n",
    "train_dl=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_dl=DataLoader(valid_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fa68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 7: Visualize a few sample images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show a small grid of images from the base_dataset to verify labels & transforms\n",
    "n_rows, n_cols = 2, 4\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 6))\n",
    "\n",
    "for ax, (img, label) in zip(axes.flatten(), base_dataset):\n",
    "    # img is a tensor in [0,1] with shape [C,H,W]\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(f\"label: {base_dataset.classes[label]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Shown a sample of images from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fce1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 8: Model\n",
    "model=nn.Sequential(\n",
    "    nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(),\n",
    "    nn.Linear(128,64), nn.ReLU(), nn.Dropout(0.5),\n",
    "    nn.Linear(64,2)\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 9: Loss & optim\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e088ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 10: Train & evaluation utilities\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    \"\"\"Compute accuracy given model logits and true labels.\"\"\"\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "def run_epoch(model, dataloader, optimizer=None, loss_fn=loss_fn, device=device):\n",
    "    \"\"\"Run one epoch over a dataloader.\n",
    "\n",
    "    If optimizer is provided, the model is set to train mode and parameters updated.\n",
    "    If optimizer is None, the model is set to eval mode and no gradients are computed.\n",
    "    Returns average loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "    else:\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_size = xb.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total_examples += batch_size\n",
    "\n",
    "    avg_loss = total_loss / max(total_examples, 1)\n",
    "    avg_acc = total_correct / max(total_examples, 1)\n",
    "\n",
    "    # always restore gradient setting to default (on)\n",
    "    torch.set_grad_enabled(True)\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def train(model, epochs, train_dl, valid_dl, optimizer=optimizer, loss_fn=loss_fn, device=device):\n",
    "    \"\"\"Train the CNN model and track metrics across epochs.\n",
    "\n",
    "    This closely follows the structure of the in-class MNIST CNN example:\n",
    "    * for each epoch, run a training pass and a validation pass\n",
    "    * record loss and accuracy for both splits\n",
    "    * print a short training log\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"valid_loss\": [],\n",
    "        \"valid_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = run_epoch(model, train_dl, optimizer=optimizer, loss_fn=loss_fn, device=device)\n",
    "        valid_loss, valid_acc = run_epoch(model, valid_dl, optimizer=None, loss_fn=loss_fn, device=device)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"valid_acc\"].append(valid_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{epochs:02d} | \"\n",
    "              f\"train loss: {train_loss:.4f} acc: {train_acc:.4f} | \"\n",
    "              f\"valid loss: {valid_loss:.4f} acc: {valid_acc:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5624a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 11–14: Train model, plot metrics, and save full model\n",
    "\n",
    "# 1) Train the model\n",
    "history = train(model, NUM_EPOCHS, train_dl, valid_dl)\n",
    "\n",
    "# 2) Plot training & validation loss / accuracy\n",
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, history[\"train_loss\"], label=\"train loss\")\n",
    "plt.plot(epochs_range, history[\"valid_loss\"], label=\"valid loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history[\"train_acc\"], label=\"train acc\")\n",
    "plt.plot(epochs_range, history[\"valid_acc\"], label=\"valid acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Simple evaluation on validation set (final metrics)\n",
    "final_valid_loss, final_valid_acc = run_epoch(model, valid_dl, optimizer=None, loss_fn=loss_fn, device=device)\n",
    "print(f\"Final validation loss: {final_valid_loss:.4f}, accuracy: {final_valid_acc:.4f}\")\n",
    "\n",
    "# 4) Save full model for later use (meets project deliverable)\n",
    "save_dir = Path(\"/content/drive/MyDrive\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_filename = f\"Group_{GROUP_NUMBER}_CNN_FullModel.ph\"\n",
    "save_path = save_dir / model_filename\n",
    "\n",
    "torch.save(model, save_path)\n",
    "print(f\"Saved full model to: {save_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
